"""
LLM Client
----------

This module provides a thin wrapper around OpenAI's ChatGPT API (or
other local models) to perform a handful of natural language tasks
required by the automation framework.  The client is responsible for
classifying test case steps into one of the supported MCP types and
translating plain‑English instructions into structured API requests or
SQL statements.

If an API key is not configured or the `use_local` flag is set in the
configuration, the client falls back to rule‑based heuristics.  Results
are cached in memory to minimise duplicate calls.  Developers are
encouraged to extend this client with additional tasks, such as
summarisation or documentation generation.
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from functools import lru_cache
from typing import Any, Dict, List, Optional

from ..config import Config
from .logger import get_logger

try:
    import openai  # type: ignore
    _openai_available = True
except ImportError:
    _openai_available = False


@dataclass
class APITranslation:
    """Representation of an API request generated by the LLM."""
    method: str
    url: str
    headers: Optional[Dict[str, str]]
    body: Optional[Any]
    expected_status: int


@dataclass
class SQLTranslation:
    """Representation of a SQL statement and optional assertion."""
    sql: str
    assertion: Optional[str]  # assertion expressed in natural language


class LLMClient:
    """Client for calling OpenAI chat completions or using heuristics."""

    def __init__(self, config: Config) -> None:
        self.config = config
        self.logger = get_logger(self.__class__.__name__)
        # Determine if we should call the API
        self.api_key: Optional[str] = config.get("openai.api_key")
        self.model: str = config.get("openai.model", "gpt-3.5-turbo")
        use_local = str(config.get("openai.use_local", "true")).lower() == "true"
        self.enabled = bool(self.api_key) and not use_local and _openai_available
        if self.enabled:
            try:
                openai.api_key = self.api_key  # type: ignore[attr-defined]
            except Exception as exc:
                self.logger.error("Failed to initialise OpenAI client: %s", exc)
                self.enabled = False

    def _chat(self, messages: List[Dict[str, str]]) -> Optional[str]:
        """Invoke the OpenAI chat completion API and return the assistant content."""
        if not self.enabled:
            return None
        try:
            response = openai.ChatCompletion.create(  # type: ignore[attr-defined]
                model=self.model,
                messages=messages,
                temperature=0.0,
            )
            return response.choices[0].message["content"]  # type: ignore[index]
        except Exception as exc:
            # Log the exception but don't raise – fallback will be used
            self.logger.error("OpenAI API call failed: %s", exc)
            return None

    @lru_cache(maxsize=128)
    def classify(self, text: str) -> str:
        """Classify a block of text into one of: ui, api, mobile, sql.

        The classifier sends a system prompt instructing the LLM to return
        strictly one of the allowed categories.  The result is lowercased.
        """
        # Fallback to heuristic if API disabled
        if not self.enabled:
            return self._heuristic_classify(text)
        prompt = (
            "You are an expert test classifier. Given a description of test steps, "
            "return exactly one word – ui, api, mobile or sql – indicating which "
            "execution context should be used. Do not return anything else."
        )
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": text.strip()},
        ]
        result = self._chat(messages)
        if result:
            category = result.strip().split()[0].lower()
            if category in {"ui", "api", "mobile", "sql"}:
                return category
        # Fallback if LLM returned unexpected output
        return self._heuristic_classify(text)

    def _heuristic_classify(self, text: str) -> str:
        """Heuristic classification based on keywords from config."""
        text_lower = text.lower()
        # Use the same keyword lists as the original router
        mapping = {
            "ui": self.config.get("router.ui_keywords", []),
            "api": self.config.get("router.api_keywords", []),
            "mobile": self.config.get("router.mobile_keywords", []),
            "sql": self.config.get("router.sql_keywords", []),
        }
        for category, keywords in mapping.items():
            for kw in keywords:
                if kw.lower() in text_lower:
                    return category
        return "api"

    @lru_cache(maxsize=128)
    def translate_api(self, command: str, base_url: str = "") -> APITranslation:
        """Translate a natural language API command into an APITranslation object.

        The OpenAI prompt asks the model to output a JSON with keys
        method, url, headers, body and expected_status.  If the model is
        unavailable or returns invalid JSON, a heuristic parser is used.
        """
        if not self.enabled:
            # Fallback to existing parser
            from .natural_language_api import parse_api_command
            req = parse_api_command(command, base_url)
            return APITranslation(req.method, req.url, req.headers, req.body, req.expected_status)
        sys_prompt = (
            "You are an assistant that converts English API commands into a JSON "
            "object describing the HTTP request. The JSON must have the keys: "
            "method (string), url (string), headers (object or null), body (object or null) and "
            "expected_status (integer). Assume a base URL of '" + base_url.rstrip("/") + "'. "
            "Never wrap the JSON in backticks or code fences."
        )
        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": command.strip()},
        ]
        result = self._chat(messages)
        if result:
            try:
                data = json.loads(result)
                method = data.get("method", "GET").upper()
                url = data.get("url", "")
                if base_url and not url.startswith("http"):
                    url = base_url.rstrip("/") + "/" + url.lstrip("/")
                headers = data.get("headers") or None
                body = data.get("body")
                expected_status = int(data.get("expected_status", 200))
                return APITranslation(method, url, headers, body, expected_status)
            except Exception as exc:
                self.logger.warning("LLM returned invalid JSON for API translation: %s", exc)
        # Fallback
        from .natural_language_api import parse_api_command
        req = parse_api_command(command, base_url)
        return APITranslation(req.method, req.url, req.headers, req.body, req.expected_status)

    @lru_cache(maxsize=128)
    def translate_sql(self, command: str) -> SQLTranslation:
        """
        Translate a natural language SQL command into a ``SQLTranslation``.

        When the OpenAI API is available and enabled, this method sends a
        prompt instructing the model to return a JSON object with ``sql`` and
        optional ``assertion`` keys.  If the API is disabled or the call
        fails, the framework falls back to a rule‑based converter defined in
        ``natural_language_sql``.  Results are cached to avoid redundant
        calls for the same input.
        """
        # Fallback to heuristic translation if LLM is disabled
        if not self.enabled:
            from .natural_language_sql import english_to_sql
            translation = english_to_sql(command)
            assertion_desc = translation.assertion.__doc__ if translation.assertion else None  # type: ignore
            return SQLTranslation(translation.sql, assertion_desc)

        # Construct the prompt for the model
        sys_prompt = (
            "You are a database expert. Convert the following plain English command "
            "into a SQL statement targeting a simple table named 'users' with columns "
            "(id integer primary key, name text). Return a JSON object with 'sql' and "
            "an optional 'assertion' key describing in English how to verify the result. "
            "Never wrap the JSON in markdown."
        )
        messages = [
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": command.strip()},
        ]
        result = self._chat(messages)
        if result:
            try:
                data = json.loads(result)
                sql = data.get("sql", command)
                assertion = data.get("assertion")
                return SQLTranslation(sql, assertion)
            except Exception as exc:
                self.logger.warning("LLM returned invalid JSON for SQL translation: %s", exc)
        # Final fallback: use the heuristic converter if the LLM output was invalid or empty
        from .natural_language_sql import english_to_sql
        translation = english_to_sql(command)
        assertion_desc = translation.assertion.__doc__ if translation.assertion else None  # type: ignore
        return SQLTranslation(translation.sql, assertion_desc)

    @lru_cache(maxsize=64)
    def suggest_ui_selector(self, description: str) -> Optional[str]:
        """
        Suggest a CSS or Playwright locator for a UI element based on the step
        description.

        This helper uses the LLM to generate a simple selector given a brief
        description of the element to click or fill.  The model is instructed
        to return *only* a selector string (no JSON, no explanations).  This
        function is cached to avoid repeating identical prompts.  If the LLM
        call fails or returns invalid data, ``None`` is returned.  A rule‑
        based heuristic is not provided because the UI MCP already attempts
        text, aria‑label and placeholder fallbacks.
        """
        if not self.enabled:
            return None
        prompt = (
            "You are a UI automation assistant. Given a brief description of a web "
            "element (e.g. the text on a button or a field label), return a Playwright "
            "locator string that could be used to select it. Prefer CSS selectors or "
            "Playwright's text= syntax. Respond with only the selector, no backticks, "
            "no explanation. If you cannot produce a selector, return an empty string."
        )
        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": description.strip()},
        ]
        result = self._chat(messages)
        if result:
            selector = result.strip().splitlines()[0].strip()
            if selector:
                return selector
        return None


__all__ = ["LLMClient", "APITranslation", "SQLTranslation"]